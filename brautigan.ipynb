{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spamhamneggs/FinalProjectCOMP6885/blob/main/brautigan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0XmZfOMMHGar"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies and setup environment\n",
        "%%capture\n",
        "\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Use specific versions optimized for the Qwen model\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    # Consolidate LLM dependencies\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "# Consolidate all NLP package installations\n",
        "!pip install -U spacy==3.7.5 spacy_syllables bertopic umap-learn hdbscan numpy pandas syllapy\n",
        "\n",
        "# Download the specific English language model required by spaCy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# IMPORTANT: After this cell executes, the kernel must be restarted\n",
        "# to load the newly installed modules into memory.\n",
        "print(\"Installation complete. PLEASE RESTART THE KERNEL NOW.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "porDIA5kHcKI",
        "outputId": "5c08160b-abef-4527-d368-e3e4a3ef40b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/haiku_dataset/haiku_dataset': File exists\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Drive Mounting and Global Variables\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a symbolic link for easy path access (as defined in your original code)\n",
        "!ln -s /content/drive/MyDrive/haiku_dataset /content/haiku_dataset\n",
        "\n",
        "# --- Global Configuration Variables ---\n",
        "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
        "# FIX: Use the symbolic link path for consistent access\n",
        "DATASET_PATH = \"/content/haiku_dataset/haiku_dataset_merged.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/haiku_suggester_unsloth\"\n",
        "\n",
        "# Training parameters\n",
        "# Path to persist BERTopic model to speed repeated runs\n",
        "TOPIC_MODEL_PATH = OUTPUT_DIR + '/bertopic_model.joblib'\n",
        "SAVE_TOPIC_MODEL = True\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GA_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "# Variables to be defined globally after training/loading\n",
        "model = None\n",
        "tokenizer = None\n",
        "LLM_READY = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aGCk9-lbLTl_"
      },
      "outputs": [],
      "source": [
        "# Cell 3: BERTopic and HaikuGrammarly Class Definition\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import warnings\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import joblib\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- CORE NLP SETUP ---\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "syllables = SpacySyllables(nlp)\n",
        "nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
        "\n",
        "class HaikuGrammarly:\n",
        "\n",
        "    def __init__(self, target_syllables: List[int] = [5, 7, 5], haiku_dataset_path: str = DATASET_PATH):\n",
        "        self.target_syllables = target_syllables\n",
        "        self.haiku_dataset_path = haiku_dataset_path\n",
        "        self.topic_model = self._initialize_bertopic()\n",
        "        self.llm_ready = globals().get('LLM_READY', False) # Check global status\n",
        "\n",
        "    def _initialize_bertopic(self):\n",
        "        \"\"\"Initializes and pre-fits the BERTopic model on the Haiku dataset.\"\"\"\n",
        "        print(\"Initializing and fitting BERTopic model on Haiku dataset...\")\n",
        "        try:\n",
        "            # Try to load a cached BERTopic model to avoid re-fitting every run\n",
        "            try:\n",
        "                if 'TOPIC_MODEL_PATH' in globals() and TOPIC_MODEL_PATH and os.path.exists(TOPIC_MODEL_PATH):\n",
        "                    print(f\"Loading cached BERTopic model from {TOPIC_MODEL_PATH}...\")\n",
        "                    topic_model = joblib.load(TOPIC_MODEL_PATH)\n",
        "                    print(\"✅ Loaded cached BERTopic model.\")\n",
        "                    return topic_model\n",
        "            except Exception as e_load:\n",
        "                print(f\"Could not load cached BERTopic model: {e_load}. Will fit a new model.\")\n",
        "            df = pd.read_csv(self.haiku_dataset_path)\n",
        "            documents = (df['line1'].fillna('') + ' ' + df['line2'].fillna('') + ' ' + df['line3'].fillna('')).tolist()\n",
        "            documents = [doc.strip() for doc in documents if len(doc.split()) >= 3]\n",
        "            vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "            topic_model = BERTopic(vectorizer_model=vectorizer_model, nr_topics=\"auto\", min_topic_size=50, verbose=False)\n",
        "            topic_model.fit(documents)\n",
        "            print(\"✅ BERTopic model initialization complete.\")\n",
        "            # Persist the model for future runs (best-effort)\n",
        "            try:\n",
        "                if 'TOPIC_MODEL_PATH' in globals() and TOPIC_MODEL_PATH and SAVE_TOPIC_MODEL:\n",
        "                    os.makedirs(os.path.dirname(TOPIC_MODEL_PATH), exist_ok=True)\n",
        "                    joblib.dump(topic_model, TOPIC_MODEL_PATH)\n",
        "                    print(f\"✅ Saved BERTopic model to {TOPIC_MODEL_PATH}\")\n",
        "            except Exception as e_save:\n",
        "                print(f\"Could not save BERTopic model: {e_save}\")\n",
        "            return topic_model\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing BERTopic: {e}. Skipping BERTopic analysis.\")\n",
        "            return None\n",
        "\n",
        "    def _get_line_syllables(self, line: str) -> int:\n",
        "        doc = nlp(line)\n",
        "        total_syllables = 0\n",
        "        for token in doc:\n",
        "            syllables_count = token._.syllables_count\n",
        "            if syllables_count is not None:\n",
        "                total_syllables += syllables_count\n",
        "        return total_syllables\n",
        "\n",
        "    def check_structure(self, lines: List[str]) -> Tuple[Dict, float]:\n",
        "        feedback = {\"structural_ok\": True, \"line_details\": []}\n",
        "        total_syllable_errors = 0\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            target = self.target_syllables[i]\n",
        "            count = self._get_line_syllables(line)\n",
        "            is_ok = (count == target)\n",
        "\n",
        "            feedback[\"line_details\"].append({\n",
        "                \"line_num\": i + 1, \"text\": line, \"count\": count, \"target\": target, \"ok\": is_ok, \"error\": target - count\n",
        "            })\n",
        "\n",
        "            if not is_ok:\n",
        "                feedback[\"structural_ok\"] = False\n",
        "                total_syllable_errors += abs(target - count)\n",
        "\n",
        "        structural_score = max(0.0, 1.0 - (total_syllable_errors / (sum(self.target_syllables) * 0.5)))\n",
        "        return feedback, structural_score\n",
        "\n",
        "    def get_semantic_coherence(self, haiku_text: str) -> float:\n",
        "        if self.topic_model is None: return 0.5\n",
        "        try:\n",
        "            topics, probabilities = self.topic_model.transform([haiku_text])\n",
        "            topic = topics[0]\n",
        "            if topic == -1: return 0.2\n",
        "            if np.isscalar(probabilities[0]):\n",
        "                coherence_score = probabilities[0]\n",
        "            elif isinstance(probabilities[0], (list, np.ndarray)) and topic < len(probabilities[0]):\n",
        "                coherence_score = probabilities[0][topic]\n",
        "            else:\n",
        "                return 0.2\n",
        "            return np.interp(coherence_score, [0.0, 1.0], [0.3, 1.0])\n",
        "        except Exception:\n",
        "            return 0.5\n",
        "\n",
        "    def check_quality_metrics(self, lines: List[str], haiku_text: str) -> Dict:\n",
        "        doc = nlp(haiku_text)\n",
        "        metrics = {\"Semantic_Coherence\": self.get_semantic_coherence(haiku_text), \"Imagery_Score\": 0.0, \"Concision_Score\": 1.0, \"Grammar_Feedback\": []}\n",
        "        for i, token in enumerate(doc):\n",
        "            if token.pos_ in [\"DET\", \"ADP\"] and token.text.lower() in [\"a\", \"an\", \"the\", \"of\", \"in\", \"on\"]: metrics[\"Concision_Score\"] -= 0.03\n",
        "            if token.pos_ == \"ADV\" and token.text.lower() in [\"very\", \"really\", \"so\"]:\n",
        "                metrics[\"Concision_Score\"] -= 0.05\n",
        "                metrics[\"Grammar_Feedback\"].append(f\"Token: '{token.text}'. Consider removing weak intensifiers for better impact.\")\n",
        "\n",
        "        metrics[\"Imagery_Score\"] = 0.4\n",
        "        metrics[\"Concision_Score\"] = max(0.0, min(1.0, metrics[\"Concision_Score\"]))\n",
        "        metrics[\"Sentiment_Balance\"] = 0.75\n",
        "        return metrics\n",
        "\n",
        "    def generate_report(self, haiku_text: str) -> Dict:\n",
        "        lines = [line.strip() for line in haiku_text.strip().split('\\n') if line.strip()]\n",
        "        if len(lines) != 3: return {\"error\": \"Haiku must have exactly three lines.\"}\n",
        "\n",
        "        structural_feedback, structural_score = self.check_structure(lines)\n",
        "        quality_metrics = self.check_quality_metrics(lines, haiku_text)\n",
        "\n",
        "        weighted_score = (structural_score * 0.40 + quality_metrics[\"Semantic_Coherence\"] * 0.25 + quality_metrics[\"Imagery_Score\"] * 0.20 + quality_metrics[\"Concision_Score\"] * 0.10 + quality_metrics[\"Sentiment_Balance\"] * 0.05)\n",
        "\n",
        "        report = {\n",
        "            \"input_haiku\": haiku_text,\n",
        "            \"weighted_quality_score\": round(weighted_score, 4),\n",
        "            \"structural_analysis\": structural_feedback,\n",
        "            \"quality_metrics\": quality_metrics,\n",
        "            \"llm_suggestion\": self._generate_llm_suggestion(haiku_text, structural_feedback, quality_metrics),\n",
        "        }\n",
        "        return report\n",
        "\n",
        "    def _generate_llm_suggestion(self, haiku_text: str, structural_feedback: Dict, quality_metrics: Dict) -> str:\n",
        "        if self.llm_ready:\n",
        "            try:\n",
        "                json_suggestions = get_json_suggestions(haiku_text)\n",
        "                return json.dumps(json_suggestions, indent=2)\n",
        "            except Exception as e:\n",
        "                return f\"Qwen/Unsloth inference failed: {e}. Falling back to placeholder.\"\n",
        "        else:\n",
        "            suggestion = \"LLM Suggestion (via Qwen3-4B Placeholder): \"\n",
        "            if not structural_feedback[\"structural_ok\"]:\n",
        "                line_errors = [d for d in structural_feedback[\"line_details\"] if not d[\"ok\"]]\n",
        "                suggestion += f\"Structure Error: Line {line_errors[0]['line_num']} is off by {line_errors[0]['error']} syllable(s). \"\n",
        "            if quality_metrics[\"Imagery_Score\"] < 0.5:\n",
        "                suggestion += \"Low Imagery Score. Use more vivid nouns. \"\n",
        "            if quality_metrics[\"Semantic_Coherence\"] < 0.5:\n",
        "                suggestion += \"Low Semantic Coherence. Ensure all three lines contribute to a single, unified image. \"\n",
        "            if suggestion == \"LLM Suggestion (via Qwen3-4B Placeholder): \":\n",
        "                suggestion += \"The Haiku is structurally sound and stylistically fair. Great job!\"\n",
        "            return suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gv68HoCHlLL",
        "outputId": "f99865f3-3d1b-4dd7-ace9-9af5482db513",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FOUND SAVED MODEL: Loading model from /content/drive/MyDrive/haiku_suggester_unsloth. Skipping training...\n",
            "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Cell 4: Data Preparation, Metric-driven Example Creation, Fine-Tuning Setup, and Training\n",
        "\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "import numpy as np\n",
        "\n",
        "global model, tokenizer, LLM_READY\n",
        "\n",
        "# --- Heuristic Syllable Counter (used as backup) ---\n",
        "def count_syllables_heuristic(line: str) -> int:\n",
        "    if not line or line.strip() == \"\":\n",
        "        return 0\n",
        "    s = re.sub(r\"[^a-zA-Z\\s]\", \" \", line)\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
        "    groups = re.findall(r\"[aeiouyAEIOUY]+\", s)\n",
        "    n = len(groups)\n",
        "    if len(s.split()) > 0 and s.endswith(\"e\") and not s.endswith(\"le\"):\n",
        "        n = max(1, n - 1)\n",
        "    return max(1, n)\n",
        "\n",
        "# Minimal fallback suggestion generator\n",
        "def generate_suggestions_for_haiku(haiku_text: str) -> Dict:\n",
        "    lines = [l.strip() for l in haiku_text.strip().splitlines() if l.strip()]\n",
        "    if len(lines) == 1 and ' / ' in lines[0]:\n",
        "        lines = [p.strip() for p in lines[0].split(' / ')]\n",
        "    while len(lines) < 3:\n",
        "        lines.append(\"\")\n",
        "    lines = lines[:3]\n",
        "    syl_counts = [count_syllables_heuristic(l) for l in lines]\n",
        "    suggestions = []\n",
        "    target = [5, 7, 5]\n",
        "    for i, (syl, tgt) in enumerate(zip(syl_counts, target)):\n",
        "        if syl != tgt:\n",
        "            suggestions.append({\"type\": \"syllable\", \"line\": i + 1, \"message\": f\"Line {i+1} has approx {syl}; revise to {tgt}.\"})\n",
        "    return {\"suggestions\": suggestions}\n",
        "\n",
        "# Define the get_json_suggestions inference function (needed globally)\n",
        "def get_json_suggestions(haiku: str, max_new_tokens=256):\n",
        "    global model, tokenizer\n",
        "    if model is None or tokenizer is None:\n",
        "        # If LLM not ready, fallback to heuristic generator\n",
        "        return generate_suggestions_for_haiku(haiku)\n",
        "\n",
        "    inference_instruction = (\n",
        "        \"You are an assistant that provides improvement suggestions for a user-provided haiku. \"\n",
        "        \"Do NOT write or invent haiku lines. Only analyze the provided haiku and return JSON only, following this schema: \"\n",
        "        \"{ \\\"input_haiku\\\": <str>, \\\"weighted_quality_score\\\": <float>, \\\"structural_analysis\\\": <obj>, \\\"quality_metrics\\\": <obj>, \\\"llm_suggestion\\\": <obj_or_str> }\"\n",
        "    )\n",
        "    prompt = f\"### Instruction:\\n{inference_instruction}\\n\\n### Input:\\n{haiku}\\n\\n### Output:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if '### Output:' in text:\n",
        "        gen = text.split('### Output:')[-1].strip()\n",
        "    else:\n",
        "        gen = text.strip()\n",
        "    try:\n",
        "        return json.loads(gen)\n",
        "    except Exception:\n",
        "        return generate_suggestions_for_haiku(haiku)\n",
        "\n",
        "# --- OPTIMIZATION CHECK: Check if the model has already been fine-tuned and saved ---\n",
        "if os.path.exists(OUTPUT_DIR) and len(os.listdir(OUTPUT_DIR)) > 0:\n",
        "    print(f\"✅ FOUND SAVED MODEL: Loading model from {OUTPUT_DIR}. Skipping training...\")\n",
        "\n",
        "    # --- Load the previously trained model directly ---\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=OUTPUT_DIR,  # Load from the saved directory\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    LLM_READY = True\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ SAVED MODEL NOT FOUND. Proceeding with initial download and fine-tuning...\")\n",
        "\n",
        "    # --- Data Loading and Metric-driven Formatting ---\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "    # keep original line columns for BERTopic (if present)\n",
        "    if all(c in df.columns for c in [\"line1\", \"line2\", \"line3\"]):\n",
        "        df[\"haiku\"] = df[[\"line1\", \"line2\", \"line3\"]].astype(str).agg(\"\\n\".join, axis=1)\n",
        "    else:\n",
        "        # fall back to using a single 'haiku' column if dataset differs\n",
        "        if \"haiku\" not in df.columns:\n",
        "            raise ValueError(\"Dataset must contain either (line1,line2,line3) or haiku column\")\n",
        "        df = df.rename(columns={\"haiku\": \"haiku\"})\n",
        "    df = df[[\"haiku\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    # Instantiate the metric checker so we can compute targets for every example\n",
        "    checker = HaikuGrammarly()\n",
        "\n",
        "    instruction = (\n",
        "        \"You are an assistant that provides improvement suggestions for a user-provided haiku. \"\n",
        "        \"Do NOT write or invent haiku lines. Only analyze the provided haiku and return JSON only, following this schema that includes a weighted_quality_score.\"\n",
        "    )\n",
        "\n",
        "    def make_example(haiku: str):\n",
        "        # Compute the metric-driven report and include the numeric target used for validation.\n",
        "        try:\n",
        "            report = checker.generate_report(haiku)\n",
        "            quality_score = report.get(\"weighted_quality_score\") if isinstance(report, dict) else None\n",
        "        except Exception:\n",
        "            # Fallback to lightweight labels if report computation fails\n",
        "            report = {\"input_haiku\": haiku, \"weighted_quality_score\": None, \"llm_suggestion\": generate_suggestions_for_haiku(haiku)}\n",
        "            quality_score = None\n",
        "        response = json.dumps(report)\n",
        "        return {\"instruction\": instruction, \"input\": haiku, \"output\": response, \"quality_score\": quality_score}\n",
        "\n",
        "    examples = [make_example(x) for x in df[\"haiku\"].tolist()]\n",
        "    hf_ds = Dataset.from_pandas(pd.DataFrame(examples))\n",
        "\n",
        "    # Create a validation split so we can track metric improvements separately\n",
        "    try:\n",
        "        hf_split = hf_ds.train_test_split(test_size=0.1, seed=42)\n",
        "        train_ds = hf_split['train']\n",
        "        val_ds = hf_split['test']\n",
        "    except Exception:\n",
        "        # fallback: use entire dataset as train if HF version doesn't support split\n",
        "        train_ds = hf_ds\n",
        "        val_ds = None\n",
        "\n",
        "    # --- Model Loading and LoRA Setup (Download occurs here on first run) ---\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=MODEL_NAME,  # Downloads Qwen on first run\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=LORA_R,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen3-instruct\")\n",
        "\n",
        "    def formatting_prompts_func(examples):\n",
        "        texts = []\n",
        "        for instr, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "            text = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Output:\\n{out}\"\n",
        "            texts.append(text)\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    train_ds = train_ds.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "    # --- SFT Trainer and Training ---\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_ds,\n",
        "        args=SFTConfig(\n",
        "            dataset_text_field=\"text\",\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GA_STEPS,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            optim=\"adamw_8bit\",\n",
        "            report_to=\"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Attempt to train a small regression head that predicts the numeric quality score directly\n",
        "    try:\n",
        "        device = getattr(model, 'device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        # Determine a way to get embeddings from the base model; try common HF attribute paths\n",
        "        base_model = None\n",
        "        if hasattr(model, 'model'):\n",
        "            base_model = model.model\n",
        "        elif hasattr(model, 'base_model'):\n",
        "            base_model = model.base_model\n",
        "        elif hasattr(model, 'transformer'):\n",
        "            base_model = model.transformer\n",
        "\n",
        "        if base_model is None:\n",
        "            print(\"Regression head training skipped: cannot access underlying transformer model to extract embeddings.\")\n",
        "        else:\n",
        "            print(\"Training regression head to predict numeric quality_score (this will be relatively fast).\")\n",
        "            base_model.eval()\n",
        "\n",
        "            # Utility to extract mean-pooled embeddings for a batch of texts\n",
        "            def get_embeddings(texts, batch_size=8):\n",
        "                embeddings = []\n",
        "                for i in range(0, len(texts), batch_size):\n",
        "                    batch = texts[i:i+batch_size]\n",
        "                    enc = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "                    with torch.no_grad():\n",
        "                        # try different forward method signatures\n",
        "                        out = None\n",
        "                        try:\n",
        "                            out = base_model(**enc, output_hidden_states=True)\n",
        "                        except Exception:\n",
        "                            try:\n",
        "                                out = base_model.forward(**enc, output_hidden_states=True)\n",
        "                            except Exception as e_f:\n",
        "                                raise RuntimeError(f\"Could not run base_model forward: {e_f}\")\n",
        "\n",
        "                        # locate last hidden state: common names\n",
        "                        if hasattr(out, 'last_hidden_state') and out.last_hidden_state is not None:\n",
        "                            last = out.last_hidden_state\n",
        "                        elif isinstance(out, (tuple, list)) and len(out) > 0:\n",
        "                            # some models return (last_hidden_state, ...)\n",
        "                            last = out[0]\n",
        "                        elif hasattr(out, 'hidden_states') and out.hidden_states is not None:\n",
        "                            last = out.hidden_states[-1]\n",
        "                        else:\n",
        "                            raise RuntimeError('Unable to find last_hidden_state from base_model output')\n",
        "\n",
        "                        # mean-pool across sequence length\n",
        "                        pooled = last.mean(dim=1).cpu()\n",
        "                        embeddings.append(pooled)\n",
        "                return torch.cat(embeddings, dim=0)\n",
        "\n",
        "            # Build regression datasets (only include examples with numeric quality_score)\n",
        "            train_texts = []\n",
        "            train_targets = []\n",
        "            if hasattr(train_ds, '__len__'):\n",
        "                for ex in train_ds:\n",
        "                    score = ex.get('quality_score')\n",
        "                    if score is not None:\n",
        "                        train_texts.append(ex.get('input') or ex.get('haiku') or ex.get('input_haiku'))\n",
        "                        train_targets.append(float(score))\n",
        "            val_texts = []\n",
        "            val_targets = []\n",
        "            if val_ds is not None:\n",
        "                for ex in val_ds:\n",
        "                    score = ex.get('quality_score')\n",
        "                    if score is not None:\n",
        "                        val_texts.append(ex.get('input') or ex.get('haiku') or ex.get('input_haiku'))\n",
        "                        val_targets.append(float(score))\n",
        "\n",
        "            if len(train_texts) == 0:\n",
        "                print(\"No numeric quality_score found in training set; skipping regression head training.\")\n",
        "            else:\n",
        "                emb_dim = None\n",
        "                # extract embeddings for small subset to get dim\n",
        "                sample_emb = get_embeddings(train_texts[:min(32, len(train_texts))])\n",
        "                emb_dim = sample_emb.shape[1]\n",
        "\n",
        "                # Define regression head\n",
        "                reg_head = torch.nn.Linear(emb_dim, 1).to(device)\n",
        "                optimizer = torch.optim.Adam(reg_head.parameters(), lr=1e-4)\n",
        "                loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "                EPOCHS = max(1, NUM_EPOCHS)\n",
        "                BATCH = 16\n",
        "                for ep in range(1, EPOCHS + 1):\n",
        "                    reg_head.train()\n",
        "                    # shuffle\n",
        "                    idxs = np.random.permutation(len(train_texts))\n",
        "                    epoch_losses = []\n",
        "                    for i in range(0, len(idxs), BATCH):\n",
        "                        batch_idx = idxs[i:i+BATCH]\n",
        "                        batch_texts = [train_texts[j] for j in batch_idx]\n",
        "                        batch_targets = torch.tensor([train_targets[j] for j in batch_idx], dtype=torch.float32).to(device)\n",
        "                        emb = get_embeddings(batch_texts)\n",
        "                        emb = emb.to(device)\n",
        "                        preds = reg_head(emb).squeeze(-1)\n",
        "                        loss = loss_fn(preds, batch_targets)\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        epoch_losses.append(loss.item())\n",
        "                    # validation\n",
        "                    reg_head.eval()\n",
        "                    val_mae = None\n",
        "                    val_mse = None\n",
        "                    if len(val_texts) > 0:\n",
        "                        with torch.no_grad():\n",
        "                            val_emb = get_embeddings(val_texts)\n",
        "                            val_emb = val_emb.to(device)\n",
        "                            val_preds = reg_head(val_emb).squeeze(-1).cpu().numpy()\n",
        "                            val_targets_np = np.array(val_targets)\n",
        "                            val_mae = float(np.mean(np.abs(val_preds - val_targets_np)))\n",
        "                            val_mse = float(np.mean((val_preds - val_targets_np)**2))\n",
        "                    avg_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n",
        "                    print(f\"Regression epoch {ep}/{EPOCHS} | train_loss={avg_loss:.6f} | val_MAE={val_mae} | val_MSE={val_mse}\")\n",
        "\n",
        "                # Save regression head weights for reuse\n",
        "                try:\n",
        "                    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "                    torch.save({'state_dict': reg_head.state_dict()}, os.path.join(OUTPUT_DIR, 'regression_head.pt'))\n",
        "                    print(f\"Saved regression head to {os.path.join(OUTPUT_DIR, 'regression_head.pt')}\")\n",
        "                except Exception as e_save:\n",
        "                    print(f\"Could not save regression head: {e_save}\")\n",
        "    except Exception as e_reg:\n",
        "        print(f\"Regression head training skipped due to error: {e_reg}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"\\n✅ Saved fine-tuned model to {OUTPUT_DIR}\")\n",
        "    LLM_READY = True\n",
        "\n",
        "    # --- Post-training evaluation on the held-out validation split (if present) ---\n",
        "    try:\n",
        "        if val_ds is None:\n",
        "            print(\"No validation split available; skipping evaluation.\")\n",
        "        else:\n",
        "            # sample up to 200 examples from validation for quicker evaluation\n",
        "            n_eval = min(200, len(val_ds))\n",
        "            sample = val_ds.select(range(n_eval)) if hasattr(val_ds, 'select') else val_ds[:n_eval]\n",
        "            actuals = []\n",
        "            preds = []\n",
        "\n",
        "            def _parse_pred_score(pred):\n",
        "                # pred may be dict (if get_json_suggestions returned JSON) or string\n",
        "                if isinstance(pred, dict):\n",
        "                    return pred.get('weighted_quality_score')\n",
        "                if isinstance(pred, str):\n",
        "                    import re as _re\n",
        "                    m = _re.search(r\"weighted_quality_score\\\"?\\s*[:=]\\s*([0-9]+\\.?[0-9]*)\", pred)\n",
        "                    if m:\n",
        "                        return float(m.group(1))\n",
        "                return None\n",
        "\n",
        "            for item in sample:\n",
        "                haiku = item.get('input') or item.get('haiku') or item.get('input_haiku')\n",
        "                target = item.get('quality_score')\n",
        "                try:\n",
        "                    pred = get_json_suggestions(haiku)\n",
        "                except Exception:\n",
        "                    pred = generate_suggestions_for_haiku(haiku)\n",
        "                p_score = _parse_pred_score(pred)\n",
        "                if target is not None and p_score is not None:\n",
        "                    actuals.append(float(target))\n",
        "                    preds.append(float(p_score))\n",
        "\n",
        "            if len(actuals) > 0:\n",
        "                actuals = np.array(actuals)\n",
        "                preds = np.array(preds)\n",
        "                mae = np.mean(np.abs(actuals - preds))\n",
        "                mse = np.mean((actuals - preds)**2)\n",
        "                print(f\"Validation eval (n={len(actuals)}): MAE={mae:.4f}, MSE={mse:.4f}\")\n",
        "            else:\n",
        "                print(\"No paired predicted/actual quality_score values available in validation sample.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Validation evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka3jAsOnlMas",
        "outputId": "f79004f3-b368-4339-b3c3-9d2ab1870635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing and fitting BERTopic model on Haiku dataset...\n",
            "Loading cached BERTopic model from /content/drive/MyDrive/haiku_suggester_unsloth/bertopic_model.joblib...\n",
            "✅ Loaded cached BERTopic model.\n",
            "\n",
            "--- Haiku Analysis Report ---\n",
            "{\n",
            "    \"input_haiku\": \"\\nThe old pond is still,\\nA frog jumps into the water,\\nA deep splash is heard.\\n\",\n",
            "    \"weighted_quality_score\": 0.8002,\n",
            "    \"structural_analysis\": {\n",
            "        \"structural_ok\": false,\n",
            "        \"line_details\": [\n",
            "            {\n",
            "                \"line_num\": 1,\n",
            "                \"text\": \"The old pond is still,\",\n",
            "                \"count\": 5,\n",
            "                \"target\": 5,\n",
            "                \"ok\": true,\n",
            "                \"error\": 0\n",
            "            },\n",
            "            {\n",
            "                \"line_num\": 2,\n",
            "                \"text\": \"A frog jumps into the water,\",\n",
            "                \"count\": 8,\n",
            "                \"target\": 7,\n",
            "                \"ok\": false,\n",
            "                \"error\": -1\n",
            "            },\n",
            "            {\n",
            "                \"line_num\": 3,\n",
            "                \"text\": \"A deep splash is heard.\",\n",
            "                \"count\": 5,\n",
            "                \"target\": 5,\n",
            "                \"ok\": true,\n",
            "                \"error\": 0\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"quality_metrics\": {\n",
            "        \"Semantic_Coherence\": 0.9668530220046017,\n",
            "        \"Imagery_Score\": 0.4,\n",
            "        \"Concision_Score\": 0.8799999999999999,\n",
            "        \"Grammar_Feedback\": [],\n",
            "        \"Sentiment_Balance\": 0.75\n",
            "    },\n",
            "    \"llm_suggestion\": \"{\\n  \\\"suggestions\\\": [\\n    {\\n      \\\"type\\\": \\\"syllable\\\",\\n      \\\"line\\\": 2,\\n      \\\"message\\\": \\\"Line 2 has approx 8; revise to 7.\\\"\\n    }\\n  ]\\n}\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Inference Test\n",
        "\n",
        "# Haiku for testing\n",
        "haiku_1 = \"\"\"\n",
        "The old pond is still,\n",
        "A frog jumps into the water,\n",
        "A deep splash is heard.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the checker (it uses the DATASET_PATH defined in Cell 2)\n",
        "checker = HaikuGrammarly()\n",
        "\n",
        "# Analyze Haiku 1\n",
        "report_1 = checker.generate_report(haiku_1)\n",
        "\n",
        "print(\"\\n--- Haiku Analysis Report ---\")\n",
        "print(json.dumps(report_1, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7z_4yjTLTmC"
      },
      "source": [
        "# How to run this notebook in Colab\n",
        "\n",
        "1. Run the first install cell (Cell 1). After it finishes, restart the Colab kernel so newly installed packages (spaCy, spacy-syllables, unsloth, etc.) are available.\n",
        "2. Re-run cells from top to bottom in order. The `HaikuGrammarly` cell initializes BERTopic and may take time on first run; a cached BERTopic model will be loaded from `OUTPUT_DIR` if present.\n",
        "3. The training cell performs supervised fine-tuning (SFT) then optionally trains a small regression head to predict the numeric `weighted_quality_score`. The regression head training is best-effort and will be skipped if your hardware or the model wrapper does not expose hidden states.\n",
        "4. After training the notebook saves: fine-tuned model and tokenizer to `OUTPUT_DIR`, and (if created) `regression_head.pt` and `bertopic_model.joblib`. Make sure `OUTPUT_DIR` points to a writable location in your Drive.\n",
        "5. If you run into memory/OOM issues: reduce `BATCH_SIZE`, set `NUM_EPOCHS` to 1, or run the notebook on a Colab Pro/Colab GPU runtime with more memory.\n",
        "\n",
        "Notes:\n",
        "- This notebook now centers the pipeline around the computed `weighted_quality_score`. Examples include the numeric target `quality_score` and training includes a dedicated regression step that logs validation MAE/MSE per epoch.\n",
        "- For stronger metric-driven optimization (directly maximizing the metric) consider creating a reward model and using RLHF or a custom loss that combines SFT and metric regression — this is more advanced and not included here to keep the notebook runnable in Colab.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}