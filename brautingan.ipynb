{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spamhamneggs/FinalProjectCOMP6885/blob/main/brautingan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XmZfOMMHGar"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies and setup environment\n",
        "%%capture\n",
        "\n",
        "# --- 1. LLM SETUP (From Group_6_NLP_Project_[Alternate].ipynb) ---\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Use specific versions optimized for the Qwen model\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    # Consolidate LLM dependencies\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "# --- 2. CORE NLP SETUP: Fix for 'No module named spacy_syllables' ---\n",
        "\n",
        "# Consolidate all NLP package installations\n",
        "!pip install spacy==3.7.2 spacy-syllables==1.0.4 bertopic umap-learn hdbscan numpy pandas\n",
        "\n",
        "# Download the specific English language model required by spaCy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# IMPORTANT: After this cell executes, the kernel must be restarted\n",
        "# to load the newly installed modules into memory.\n",
        "print(\"Installation complete. PLEASE RESTART THE KERNEL NOW.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Drive Mounting and Global Variables\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a symbolic link for easy path access (as defined in your original code)\n",
        "!ln -s /content/drive/MyDrive/haiku_dataset /content/haiku_dataset\n",
        "\n",
        "# --- Global Configuration Variables ---\n",
        "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
        "# FIX: Use the symbolic link path for consistent access\n",
        "DATASET_PATH = \"/content/haiku_dataset/haiku_dataset_merged.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/haiku_suggester_unsloth\"\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GA_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "# Variables to be defined globally after training/loading\n",
        "model = None\n",
        "tokenizer = None\n",
        "LLM_READY = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "porDIA5kHcKI",
        "outputId": "ecf4b972-c53c-4d1a-bd6c-ec134f308401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/haiku_dataset/haiku_dataset': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Preparation, Fine-Tuning Setup, and Training (Optimized)\n",
        "\n",
        "import re, json, pandas as pd\n",
        "from typing import Dict\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "global model, tokenizer, LLM_READY\n",
        "\n",
        "# --- Heuristic Syllable Counter (used for creating mock training labels) ---\n",
        "def count_syllables_heuristic(line: str) -> int:\n",
        "    # [Insert your count_syllables_heuristic function code here]\n",
        "    if not line or line.strip() == \"\": return 0\n",
        "    s = re.sub(r\"[^a-zA-Z\\s]\", \" \", line)\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
        "    groups = re.findall(r\"[aeiouyAEIOUY]+\", s)\n",
        "    n = len(groups)\n",
        "    if len(s.split()) > 0 and s.endswith(\"e\") and not s.endswith(\"le\"): n = max(1, n - 1)\n",
        "    return max(1, n)\n",
        "\n",
        "# --- Mock Suggestion Generator (for creating training examples) ---\n",
        "def generate_suggestions_for_haiku(haiku_text: str) -> Dict:\n",
        "    lines = [l.strip() for l in haiku_text.strip().splitlines() if l.strip()]\n",
        "    if len(lines) == 1 and ' / ' in lines[0]:\n",
        "        lines = [p.strip() for p in lines[0].split(' / ')]\n",
        "    while len(lines) < 3: lines.append(\"\")\n",
        "    lines = lines[:3]\n",
        "    syl_counts = [count_syllables_heuristic(l) for l in lines]\n",
        "    suggestions = []\n",
        "    target = [5, 7, 5]\n",
        "    for i, (syl, tgt) in enumerate(zip(syl_counts, target)):\n",
        "        if syl != tgt:\n",
        "            suggestions.append({\"type\": \"syllable\",\"line\": i+1,\"message\": f\"Line {i+1} has approx {syl}; revise to {tgt}.\"})\n",
        "    return {\"suggestions\": suggestions}\n",
        "\n",
        "# Define the get_json_suggestions inference function (needed globally)\n",
        "# This function is now defined regardless of whether the model was trained or loaded.\n",
        "def get_json_suggestions(haiku: str, max_new_tokens=256):\n",
        "    global model, tokenizer\n",
        "    inference_instruction = (\n",
        "        \"You are an assistant that provides improvement suggestions for a user-provided haiku. \"\n",
        "        \"Do NOT write or invent haiku lines. Only analyze the provided haiku and return JSON only, following this schema: \"\n",
        "        \"{ \"\n",
        "            \"\\\"suggestions\\\": [ \"\n",
        "                \"{ \"\n",
        "                    \"\\\"type\\\": \\\"syllable\\\" | \\\"wording\\\" | \\\"readability\\\" | \\\"imagery\\\" | \\\"conciseness\\\" | \\\"sound\\\" | \\\"tone\\\" | \\\"style\\\" | \\\"grammar\\\", \"\n",
        "                    \"\\\"line\\\": <int, optional>, \"\n",
        "                    \"\\\"message\\\": <string>, \"\n",
        "                    \"\\\"replacement\\\": <string or null> \"\n",
        "                \"} \"\n",
        "            \"] \"\n",
        "        \"} \"\n",
        "        \"Consider not only syllable structure, but also readability, vivid imagery, word economy, rhythm/sound, tone consistency, style polish, and grammar or punctuation issues.\"\n",
        "    )\n",
        "    prompt = f\"### Instruction:\\n{inference_instruction}\\n\\n### Input:\\n{haiku}\\n\\n### Output:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if '### Output:' in text:\n",
        "        gen = text.split('### Output:')[-1].strip()\n",
        "    else:\n",
        "        gen = text.strip()\n",
        "    try:\n",
        "        return json.loads(gen)\n",
        "    except:\n",
        "        # Fallback to the heuristic counter if LLM output is garbage\n",
        "        return generate_suggestions_for_haiku(haiku)\n",
        "\n",
        "\n",
        "# --- OPTIMIZATION CHECK: Check if the model has already been fine-tuned and saved ---\n",
        "if os.path.exists(OUTPUT_DIR) and len(os.listdir(OUTPUT_DIR)) > 0:\n",
        "    print(f\"✅ FOUND SAVED MODEL: Loading model from {OUTPUT_DIR}. Skipping training...\")\n",
        "\n",
        "    # --- Load the previously trained model directly ---\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = OUTPUT_DIR, # Load from the saved directory\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    LLM_READY = True\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ SAVED MODEL NOT FOUND. Proceeding with initial download and fine-tuning...\")\n",
        "\n",
        "    # --- Data Loading and Formatting ---\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "    df[\"haiku\"] = df[[\"line1\", \"line2\", \"line3\"]].astype(str).agg(\"\\n\".join, axis=1)\n",
        "    df = df[[\"haiku\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    instruction = (\n",
        "        \"You are an assistant that provides improvement suggestions for a user-provided haiku. \"\n",
        "        \"Do NOT write or invent haiku lines. Only analyze the provided haiku and return JSON only, following this schema: \"\n",
        "        # ... (Rest of your instruction text)\n",
        "    )\n",
        "\n",
        "    def make_example(haiku: str):\n",
        "        response = json.dumps(generate_suggestions_for_haiku(haiku))\n",
        "        return {\"instruction\": instruction, \"input\": haiku, \"output\": response}\n",
        "\n",
        "    examples = [make_example(x) for x in df[\"haiku\"].tolist()]\n",
        "    hf_ds = Dataset.from_pandas(pd.DataFrame(examples))\n",
        "\n",
        "    # --- Model Loading and LoRA Setup (Download occurs here on first run) ---\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = MODEL_NAME, # Downloads Qwen on first run\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = LORA_R,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "        lora_alpha = LORA_ALPHA,\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template = \"qwen3-instruct\")\n",
        "\n",
        "    def formatting_prompts_func(examples):\n",
        "        texts = []\n",
        "        for instr, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "            text = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Output:\\n{out}\"\n",
        "            texts.append(text)\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    hf_ds = hf_ds.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "    # --- SFT Trainer and Training ---\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = hf_ds,\n",
        "        args = SFTConfig(\n",
        "            dataset_text_field = \"text\",\n",
        "            per_device_train_batch_size = BATCH_SIZE,\n",
        "            gradient_accumulation_steps = GA_STEPS,\n",
        "            num_train_epochs = NUM_EPOCHS,\n",
        "            learning_rate = LEARNING_RATE,\n",
        "            optim = \"adamw_8bit\",\n",
        "            report_to = \"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"\\n✅ Saved fine-tuned model to {OUTPUT_DIR}\")\n",
        "    LLM_READY = True\n",
        "\n",
        "# --- End of Optimization Check ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gv68HoCHlLL",
        "outputId": "64d8bce3-40b5-49c8-e21e-925f0b1991c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FOUND SAVED MODEL: Loading model from /content/drive/MyDrive/haiku_suggester_unsloth. Skipping training...\n",
            "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9cdecff",
        "outputId": "5b0154d3-1d3f-4c34-d624-19c0ce44c719"
      },
      "source": [
        "# Reinstall spacy and spacy-syllables and download the language model\n",
        "!pip install spacy==3.7.2 \"spacy-syllables>=3.0.0,<4.0.0\"\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.2 in /usr/local/lib/python3.12/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-syllables<4.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (3.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.26.4)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from spacy-syllables<4.0.0,>=3.0.0) (0.17.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.10.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy==3.7.2) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.1)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\n",
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz because of HTTP error 404 Client Error: Not Found for url: https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz for URL https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: BERTopic and HaikuGrammarly Class Definition\n",
        "\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import warnings\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple # Import List, Dict, and Tuple\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- CORE NLP SETUP ---\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "syllables = SpacySyllables(nlp)\n",
        "nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
        "\n",
        "# --- CUSTOM LEXICON (Kigo List) ---\n",
        "KIGO_LIST = [\n",
        "    \"spring\", \"summer\", \"fall\", \"autumn\", \"winter\",\n",
        "    \"snow\", \"rain\", \"wind\", \"moon\", \"flower\", \"stream\",\n",
        "    \"frog\", \"cicada\", \"cherry blossom\", \"mist\", \"sun\", \"leaf\"\n",
        "]\n",
        "\n",
        "class HaikuGrammarly:\n",
        "\n",
        "    def __init__(self, target_syllables: List[int] = [5, 7, 5], haiku_dataset_path: str = DATASET_PATH):\n",
        "        self.target_syllables = target_syllables\n",
        "        self.kigo_list = set([word.lower() for word in KIGO_LIST])\n",
        "        self.haiku_dataset_path = haiku_dataset_path\n",
        "        self.topic_model = self._initialize_bertopic()\n",
        "        self.llm_ready = globals().get('LLM_READY', False) # Check global status\n",
        "\n",
        "    def _initialize_bertopic(self):\n",
        "        \"\"\"Initializes and pre-fits the BERTopic model on the Haiku dataset.\"\"\"\n",
        "        print(\"Initializing and fitting BERTopic model on Haiku dataset...\")\n",
        "        try:\n",
        "            df = pd.read_csv(self.haiku_dataset_path)\n",
        "            documents = (df['line1'].fillna('') + ' ' + df['line2'].fillna('') + ' ' + df['line3'].fillna('')).tolist()\n",
        "            documents = [doc.strip() for doc in documents if len(doc.split()) >= 3]\n",
        "            vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "            topic_model = BERTopic(vectorizer_model=vectorizer_model, nr_topics=\"auto\", min_topic_size=50, verbose=False)\n",
        "            topic_model.fit(documents)\n",
        "            print(\"✅ BERTopic model initialization complete.\")\n",
        "            return topic_model\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing BERTopic: {e}. Skipping BERTopic analysis.\")\n",
        "            return None\n",
        "\n",
        "    def _get_line_syllables(self, line: str) -> int:\n",
        "        doc = nlp(line)\n",
        "        total_syllables = 0\n",
        "        for token in doc:\n",
        "            syllables_count = token._.syllables_count\n",
        "            if syllables_count is not None:\n",
        "                total_syllables += syllables_count\n",
        "        return total_syllables\n",
        "\n",
        "    def check_structure(self, lines: List[str]) -> Tuple[Dict, float]:\n",
        "        # [Structural Check Logic]... (As defined in previous response)\n",
        "        feedback = {\"structural_ok\": True, \"line_details\": []}\n",
        "        total_syllable_errors = 0\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            target = self.target_syllables[i]\n",
        "            count = self._get_line_syllables(line)\n",
        "            is_ok = (count == target)\n",
        "\n",
        "            feedback[\"line_details\"].append({\n",
        "                \"line_num\": i + 1, \"text\": line, \"count\": count, \"target\": target, \"ok\": is_ok, \"error\": target - count\n",
        "            })\n",
        "\n",
        "            if not is_ok:\n",
        "                feedback[\"structural_ok\"] = False\n",
        "                total_syllable_errors += abs(target - count)\n",
        "\n",
        "        structural_score = max(0.0, 1.0 - (total_syllable_errors / (sum(self.target_syllables) * 0.5)))\n",
        "        return feedback, structural_score\n",
        "\n",
        "    def get_semantic_coherence(self, haiku_text: str) -> float:\n",
        "        # [BERTopic Metric Logic]... (As defined in previous response)\n",
        "        if self.topic_model is None: return 0.5\n",
        "        topics, probabilities = self.topic_model.transform([haiku_text])\n",
        "        topic = topics[0]\n",
        "        if topic == -1: return 0.2\n",
        "        # Check if probabilities[0] is a scalar or an array/list and if topic is a valid index\n",
        "        if np.isscalar(probabilities[0]):\n",
        "             # If scalar, it means there was only one probability returned\n",
        "             coherence_score = probabilities[0]\n",
        "        elif isinstance(probabilities[0], (list, np.ndarray)) and topic < len(probabilities[0]):\n",
        "             # Otherwise, index with the topic if it's a valid index\n",
        "             coherence_score = probabilities[0][topic]\n",
        "        else:\n",
        "            # Fallback if the structure is unexpected\n",
        "            return 0.2\n",
        "\n",
        "        return np.interp(coherence_score, [0.0, 1.0], [0.3, 1.0])\n",
        "\n",
        "    def check_quality_metrics(self, lines: List[str], haiku_text: str) -> Dict:\n",
        "        # [Qualitative/Grammarly Logic]... (As defined in previous response)\n",
        "        doc = nlp(haiku_text)\n",
        "        metrics = {\"Semantic_Coherence\": self.get_semantic_coherence(haiku_text), \"Imagery_Score\": 0.0, \"Concision_Score\": 1.0, \"Grammar_Feedback\": []}\n",
        "        found_kigo = False\n",
        "        for i, token in enumerate(doc):\n",
        "            if token.text.lower() in self.kigo_list: found_kigo = True\n",
        "            if token.pos_ in [\"DET\", \"ADP\"] and token.text.lower() in [\"a\", \"an\", \"the\", \"of\", \"in\", \"on\"]: metrics[\"Concision_Score\"] -= 0.03\n",
        "            if token.pos_ == \"ADV\" and token.text.lower() in [\"very\", \"really\", \"so\"]:\n",
        "                metrics[\"Concision_Score\"] -= 0.05\n",
        "                metrics[\"Grammar_Feedback\"].append(f\"Token: '{token.text}'. Consider removing weak intensifiers for better impact.\")\n",
        "\n",
        "        metrics[\"Imagery_Score\"] = 0.9 if found_kigo else 0.4\n",
        "        metrics[\"Concision_Score\"] = max(0.0, min(1.0, metrics[\"Concision_Score\"]))\n",
        "        metrics[\"Sentiment_Balance\"] = 0.75\n",
        "        return metrics\n",
        "\n",
        "    def generate_report(self, haiku_text: str) -> Dict:\n",
        "        # [Report Generation Logic]...\n",
        "        lines = [line.strip() for line in haiku_text.strip().split('\\n') if line.strip()]\n",
        "        if len(lines) != 3: return {\"error\": \"Haiku must have exactly three lines.\"}\n",
        "\n",
        "        structural_feedback, structural_score = self.check_structure(lines)\n",
        "        quality_metrics = self.check_quality_metrics(lines, haiku_text)\n",
        "\n",
        "        weighted_score = (structural_score * 0.40 + quality_metrics[\"Semantic_Coherence\"] * 0.25 + quality_metrics[\"Imagery_Score\"] * 0.20 + quality_metrics[\"Concision_Score\"] * 0.10 + quality_metrics[\"Sentiment_Balance\"] * 0.05)\n",
        "\n",
        "        report = {\n",
        "            \"input_haiku\": haiku_text,\n",
        "            \"weighted_quality_score\": round(weighted_score, 4),\n",
        "            \"structural_analysis\": structural_feedback,\n",
        "            \"quality_metrics\": quality_metrics,\n",
        "            \"llm_suggestion\": self._generate_llm_suggestion(haiku_text, structural_feedback, quality_metrics)\n",
        "        }\n",
        "        return report\n",
        "\n",
        "    def _generate_llm_suggestion(self, haiku_text: str, structural_feedback: Dict, quality_metrics: Dict) -> str:\n",
        "        \"\"\"\n",
        "        [III. Qwen/Unsloth LLM Assistance] - Calls the inference function or placeholder.\n",
        "        \"\"\"\n",
        "        if self.llm_ready:\n",
        "            # Call the function defined in Cell 3's setup\n",
        "            try:\n",
        "                # get_json_suggestions is assumed to be defined globally after training\n",
        "                json_suggestions = get_json_suggestions(haiku_text)\n",
        "                return json.dumps(json_suggestions, indent=2)\n",
        "            except Exception as e:\n",
        "                return f\"Qwen/Unsloth inference failed: {e}. Falling back to placeholder.\"\n",
        "        else:\n",
        "            # --- SMART PLACEHOLDER LOGIC ---\n",
        "            suggestion = \"LLM Suggestion (via Qwen3-4B Placeholder): \"\n",
        "            if not structural_feedback[\"structural_ok\"]:\n",
        "                line_errors = [d for d in structural_feedback[\"line_details\"] if not d[\"ok\"]]\n",
        "                suggestion += f\"Structure Error: Line {line_errors[0]['line_num']} is off by {line_errors[0]['error']} syllable(s). \"\n",
        "            if quality_metrics[\"Imagery_Score\"] < 0.5:\n",
        "                suggestion += \"Low Imagery Score. Use a kigo (seasonal word) or more vivid nouns. \"\n",
        "            if quality_metrics[\"Semantic_Coherence\"] < 0.5:\n",
        "                 suggestion += \"Low Semantic Coherence. Ensure all three lines contribute to a single, unified image. \"\n",
        "            if suggestion == \"LLM Suggestion (via Qwen3-4B Placeholder): \":\n",
        "                suggestion += \"The Haiku is structurally sound and stylistically fair. Great job!\"\n",
        "            return suggestion"
      ],
      "metadata": {
        "id": "mJPabfpLNmoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Inference Test\n",
        "\n",
        "# Haiku for testing\n",
        "haiku_1 = \"\"\"\n",
        "The old pond is still,\n",
        "A frog jumps into the water,\n",
        "A deep splash is heard.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the checker (it uses the DATASET_PATH defined in Cell 2)\n",
        "checker = HaikuGrammarly()\n",
        "\n",
        "# Analyze Haiku 1\n",
        "report_1 = checker.generate_report(haiku_1)\n",
        "\n",
        "print(\"\\n--- Haiku Analysis Report ---\")\n",
        "print(json.dumps(report_1, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka3jAsOnlMas",
        "outputId": "57bef2e5-4256-4d10-d015-bc2686b83925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing and fitting BERTopic model on Haiku dataset...\n",
            "✅ BERTopic model initialization complete.\n",
            "\n",
            "--- Haiku Analysis Report ---\n",
            "{\n",
            "    \"input_haiku\": \"\\nThe old pond is still,\\nA frog jumps into the water,\\nA deep splash is heard.\\n\",\n",
            "    \"weighted_quality_score\": 0.9084,\n",
            "    \"structural_analysis\": {\n",
            "        \"structural_ok\": false,\n",
            "        \"line_details\": [\n",
            "            {\n",
            "                \"line_num\": 1,\n",
            "                \"text\": \"The old pond is still,\",\n",
            "                \"count\": 5,\n",
            "                \"target\": 5,\n",
            "                \"ok\": true,\n",
            "                \"error\": 0\n",
            "            },\n",
            "            {\n",
            "                \"line_num\": 2,\n",
            "                \"text\": \"A frog jumps into the water,\",\n",
            "                \"count\": 8,\n",
            "                \"target\": 7,\n",
            "                \"ok\": false,\n",
            "                \"error\": -1\n",
            "            },\n",
            "            {\n",
            "                \"line_num\": 3,\n",
            "                \"text\": \"A deep splash is heard.\",\n",
            "                \"count\": 5,\n",
            "                \"target\": 5,\n",
            "                \"ok\": true,\n",
            "                \"error\": 0\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"quality_metrics\": {\n",
            "        \"Semantic_Coherence\": 1.0,\n",
            "        \"Imagery_Score\": 0.9,\n",
            "        \"Concision_Score\": 0.8799999999999999,\n",
            "        \"Grammar_Feedback\": [],\n",
            "        \"Sentiment_Balance\": 0.75\n",
            "    },\n",
            "    \"llm_suggestion\": \"Qwen/Unsloth inference failed: name 'generate_suggestions_for_haiku' is not defined. Falling back to placeholder.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}